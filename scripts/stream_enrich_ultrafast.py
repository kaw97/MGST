#!/usr/bin/env python3
"""
Ultra-fast comprehensive streaming enrichment using pre-built sector index.
Expected to process 4M records in minutes instead of days.
Requires complete_sector_index.pkl built by build_full_sector_index.py
"""

import json
import pickle
import re
import threading
import queue
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Any
import sys
import time

@dataclass
class StellarData:
    """Container for stellar characteristics."""
    spectral_class: str = ""
    solar_masses: float = 0.0
    surface_temperature: float = 0.0
    luminosity: str = ""
    age: int = 0
    main_star_count: int = 0
    system_type: str = ""

@dataclass
class BodyData:
    """Container for specific body characteristics where discovery was made."""
    distance_to_arrival: float = 0.0
    body_type: str = ""
    planet_class: str = ""
    mass_em: float = 0.0
    radius: float = 0.0
    surface_gravity: float = 0.0
    surface_temperature: float = 0.0
    atmosphere: str = ""
    terraforming_state: str = ""
    orbital_period: float = 0.0
    semi_major_axis: float = 0.0
    body_found: bool = False

class UltraFastEnricher:
    """Ultra-fast enricher using pre-built sector index for instant lookups."""

    def __init__(self, index_path: Path, output_dir: Path, num_workers: int = 12):
        self.index_path = index_path
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.num_workers = num_workers

        # Load the complete sector index into memory
        print("üöÄ Loading complete sector index for ultra-fast processing...")
        start_time = time.time()

        with open(index_path, 'rb') as f:
            self.sector_index = pickle.load(f)

        load_time = time.time() - start_time
        total_systems = sum(len(systems) for systems in self.sector_index.values())

        print(f"‚úÖ Index loaded in {load_time:.1f} seconds!")
        print(f"üìä {len(self.sector_index):,} sectors, {total_systems:,} systems ready for instant lookup")

        # Thread-safe data structures
        self.stats = {
            'processed': 0,
            'systematic': 0,
            'enriched': 0,
            'body_found': 0,
            'body_not_found': 0,
            'instant_lookups': 0
        }
        self.stats_lock = threading.Lock()
        self.output_lock = threading.Lock()

    def parse_system_name(self, system_name: str) -> Tuple[Optional[str], Optional[str]]:
        """Parse system name to extract sector and mass code."""
        mass_code_pattern = r'\b([A-Z]{2}-[A-Z])\b'
        match = re.search(mass_code_pattern, system_name)

        if match:
            mass_code = match.group(1)
            mass_code_start = match.start()
            sector_name = system_name[:mass_code_start].strip()
            return sector_name, mass_code

        return None, None

    def find_system_instant(self, sector_name: str, target_system_name: str) -> Optional[Dict[str, Any]]:
        """Find system using pre-loaded index - INSTANT lookup!"""
        with self.stats_lock:
            self.stats['instant_lookups'] += 1

        # Convert sector name to match index keys
        safe_sector_name = re.sub(r'[^a-zA-Z0-9_-]', '_', sector_name)
        safe_sector_name = re.sub(r'_+', '_', safe_sector_name).strip('_')

        sector_systems = self.sector_index.get(safe_sector_name, {})
        return sector_systems.get(target_system_name)

    def extract_stellar_characteristics(self, system_data: Dict[str, Any]) -> StellarData:
        """Extract stellar characteristics from system data."""
        stellar_data = StellarData()

        bodies = system_data.get('bodies', [])
        main_stars = []

        for body in bodies:
            if body.get('type') == 'Star' and body.get('mainStar', False):
                main_stars.append(body)

        stellar_data.main_star_count = len(main_stars)
        stellar_data.system_type = "single" if len(main_stars) == 1 else "multi"

        # Get primary star characteristics
        if main_stars:
            primary_star = main_stars[0]  # Use first main star
            stellar_data.spectral_class = primary_star.get('spectralClass', '')
            stellar_data.solar_masses = primary_star.get('solarMasses', 0.0)
            stellar_data.surface_temperature = primary_star.get('surfaceTemperature', 0.0)
            stellar_data.luminosity = primary_star.get('luminosity', '')
            stellar_data.age = primary_star.get('age', 0)

        return stellar_data

    def find_specific_body(self, system_data: Dict[str, Any], target_body_name: str) -> BodyData:
        """Find the specific planetary body where the discovery was made."""
        body_data = BodyData()

        # Handle None or empty target_body_name
        if not target_body_name:
            return body_data

        bodies = system_data.get('bodies', [])

        # Strategy 1: Exact match using 'name' field
        for body in bodies:
            body_name = body.get('name', '')
            if body_name == target_body_name:
                return self._extract_body_data(body, True)

        # Strategy 2: Partial match for complex body names
        for body in bodies:
            body_name = body.get('name', '')
            if not body_name:
                continue

            if target_body_name.startswith(body_name) or body_name.startswith(target_body_name):
                return self._extract_body_data(body, True)

        # Strategy 3: Look for planets/moons by bodyId
        target_numbers = re.findall(r'\d+', target_body_name)
        if target_numbers:
            target_body_id = int(target_numbers[-1])
            for body in bodies:
                body_id = body.get('bodyId', -1)
                body_type = body.get('type', '')

                if body_id == target_body_id and body_type not in ['Star']:
                    return self._extract_body_data(body, True)

        # Strategy 4: If all else fails, look for any non-star body
        for body in bodies:
            body_type = body.get('type', '')
            if body_type in ['Planet']:
                return self._extract_body_data(body, True)

        return body_data

    def _extract_body_data(self, body: Dict[str, Any], found: bool) -> BodyData:
        """Extract data from a body dictionary."""
        body_data = BodyData()
        body_data.body_found = found

        if found:
            body_data.distance_to_arrival = body.get('distanceToArrival', 0.0)
            body_data.body_type = body.get('type', '')
            body_data.planet_class = body.get('planetClass', '')
            body_data.mass_em = body.get('massEM', 0.0)
            body_data.radius = body.get('radius', 0.0)
            body_data.surface_gravity = body.get('surfaceGravity', 0.0)
            body_data.surface_temperature = body.get('surfaceTemperature', 0.0)
            body_data.atmosphere = body.get('atmosphereType', '')
            body_data.terraforming_state = body.get('terraformingState', '')
            body_data.orbital_period = body.get('orbitalPeriod', 0.0)
            body_data.semi_major_axis = body.get('semiMajorAxis', 0.0)

        return body_data

    def enrich_codex_entry(self, entry: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Enrich a single codex entry with ultra-fast instant lookups."""
        system_name = entry.get('system', '')
        if not system_name:
            return None

        # Only process systematic naming systems
        sector_name, mass_code = self.parse_system_name(system_name)
        if not sector_name or not mass_code:
            return None

        # INSTANT system lookup using pre-loaded index
        system_data = self.find_system_instant(sector_name, system_name)
        if not system_data:
            return None

        # Extract stellar characteristics
        stellar_data = self.extract_stellar_characteristics(system_data)

        # Extract specific body characteristics
        target_body_name = entry.get('body', '')
        body_data = self.find_specific_body(system_data, target_body_name)

        # Update statistics
        with self.stats_lock:
            if body_data.body_found:
                self.stats['body_found'] += 1
            else:
                self.stats['body_not_found'] += 1

        # Create enriched entry
        enriched_entry = entry.copy()
        enriched_entry.update({
            'sector_name': sector_name,
            'mass_code': mass_code,
            'star_spectral_class': stellar_data.spectral_class,
            'star_solar_masses': stellar_data.solar_masses,
            'star_surface_temperature': stellar_data.surface_temperature,
            'star_luminosity': stellar_data.luminosity,
            'star_age': stellar_data.age,
            'star_count': stellar_data.main_star_count,
            'system_type': stellar_data.system_type,
            'body_distance_to_arrival': body_data.distance_to_arrival,
            'body_type': body_data.body_type,
            'body_planet_class': body_data.planet_class,
            'body_mass_earth': body_data.mass_em,
            'body_radius_km': body_data.radius,
            'body_surface_gravity': body_data.surface_gravity,
            'body_surface_temperature': body_data.surface_temperature,
            'body_atmosphere': body_data.atmosphere,
            'body_terraforming_state': body_data.terraforming_state,
            'body_orbital_period_days': body_data.orbital_period,
            'body_orbital_distance_ls': body_data.semi_major_axis,
            'body_found_in_database': body_data.body_found,
            'enrichment_timestamp': datetime.now().isoformat()
        })

        return enriched_entry

    def ultrafast_worker_thread(self, entry_queue: queue.Queue, result_queue: queue.Queue):
        """Ultra-fast worker thread with instant lookups."""
        batch_size = 50  # Larger batches for ultra-fast processing

        while True:
            batch = []

            # Collect a batch of entries
            for _ in range(batch_size):
                try:
                    entry = entry_queue.get(timeout=1)
                    if entry is None:  # Sentinel
                        if batch:
                            self._process_ultrafast_batch(batch, result_queue)
                        return
                    batch.append(entry)
                except queue.Empty:
                    break

            if batch:
                self._process_ultrafast_batch(batch, result_queue)
                for _ in batch:
                    entry_queue.task_done()

    def _process_ultrafast_batch(self, batch: List[Dict[str, Any]], result_queue: queue.Queue):
        """Process a batch with ultra-fast instant lookups."""
        for entry in batch:
            try:
                enriched = self.enrich_codex_entry(entry)
                if enriched:
                    result_queue.put(enriched)
            except Exception as e:
                print(f"Ultra-fast processing error: {e}", file=sys.stderr)

    def stream_process_codex_ultrafast(self, codex_path: Path, output_file: Path, max_entries: Optional[int] = None):
        """Ultra-fast multithreaded stream processing with instant lookups."""

        start_time = time.time()

        print(f"üöÄ Starting ULTRA-FAST comprehensive species analysis enrichment")
        print(f"‚ö° Workers: {self.num_workers}")
        print(f"üíæ Using pre-loaded sector index for INSTANT lookups")
        print(f"üìÇ Input: {codex_path}")
        print(f"üìÅ Output: {output_file}")
        print("üéØ Expected: 50-100x speed improvement with instant system lookups")
        print("=" * 80)

        entries_read = 0
        entries_processed = 0
        systematic_found = 0
        enriched_count = 0

        # Create queues for ultra-fast multithreading
        entry_queue = queue.Queue(maxsize=5000)  # Large queue for ultra-fast processing
        result_queue = queue.Queue()

        # Start ultra-fast worker threads
        workers = []
        for i in range(self.num_workers):
            worker = threading.Thread(target=self.ultrafast_worker_thread, args=(entry_queue, result_queue))
            worker.daemon = True
            worker.start()
            workers.append(worker)

        # Start ultra-fast output writer thread
        def ultrafast_output_writer():
            nonlocal enriched_count
            with open(output_file, 'w', encoding='utf-8') as outfile:
                while True:
                    try:
                        enriched = result_queue.get(timeout=2)
                        if enriched is None:  # Sentinel to stop
                            break

                        with self.output_lock:
                            json.dump(enriched, outfile, ensure_ascii=False)
                            outfile.write('\n')
                            enriched_count += 1

                            # Real-time enrichment updates
                            if enriched_count % 500 == 0 or enriched_count <= 10:
                                elapsed = time.time() - start_time
                                enrichment_rate = enriched_count / elapsed * 60  # per minute
                                processing_rate = entries_processed / elapsed * 60  # per minute
                                print(f"‚ö° Enriched #{enriched_count:,}: {enriched.get('system', 'Unknown')} "
                                      f"(Rate: {enrichment_rate:.0f}/min, Processing: {processing_rate:.0f}/min)")
                                sys.stdout.flush()

                        result_queue.task_done()
                    except queue.Empty:
                        continue

        writer_thread = threading.Thread(target=ultrafast_output_writer)
        writer_thread.daemon = True
        writer_thread.start()

        # Read and queue entries for ultra-fast processing
        with open(codex_path, 'r', encoding='utf-8') as infile:
            for line in infile:
                if max_entries and entries_read >= max_entries:
                    break

                entries_read += 1

                try:
                    entry = json.loads(line.strip())
                    entries_processed += 1

                    # Check if systematic
                    system_name = entry.get('system', '')
                    if system_name:
                        sector_name, mass_code = self.parse_system_name(system_name)
                        if sector_name and mass_code:
                            systematic_found += 1
                            entry_queue.put(entry)

                    # Progress reporting with ultra-fast stats
                    if entries_processed % 10000 == 0:
                        systematic_pct = (systematic_found / entries_processed) * 100
                        elapsed = time.time() - start_time
                        processing_rate = entries_processed / elapsed * 60  # per minute

                        with self.stats_lock:
                            lookups_per_min = self.stats['instant_lookups'] / elapsed * 60
                            body_found_pct = (self.stats['body_found'] / max(1, enriched_count)) * 100

                            print(f"üöÄ Processed {entries_processed:,} entries ({processing_rate:.0f}/min), "
                                  f"systematic {systematic_found:,} ({systematic_pct:.1f}%), "
                                  f"enriched {enriched_count:,}, "
                                  f"instant lookups {lookups_per_min:.0f}/min, "
                                  f"bodies found {self.stats['body_found']:,} ({body_found_pct:.1f}%)")
                            sys.stdout.flush()

                except json.JSONDecodeError:
                    continue

        # Signal workers to stop
        for _ in range(self.num_workers):
            entry_queue.put(None)

        # Wait for all work to complete
        entry_queue.join()
        for worker in workers:
            worker.join()

        # Signal output writer to stop
        result_queue.put(None)
        writer_thread.join()

        elapsed_time = time.time() - start_time

        # Final ultra-fast statistics
        print("\nüéâ ULTRA-FAST COMPREHENSIVE SPECIES ANALYSIS ENRICHMENT COMPLETE!")
        print("=" * 70)
        print(f"‚ö° Total runtime: {elapsed_time/3600:.2f} hours ({elapsed_time/60:.1f} minutes)")
        print(f"üîß Workers used: {self.num_workers}")
        print(f"üìñ Total entries read: {entries_read:,}")
        print(f"‚ö° Total entries processed: {entries_processed:,}")
        print(f"üöÄ Processing rate: {entries_processed/elapsed_time*60:.0f} entries/minute")
        print(f"üéØ Systematic naming found: {systematic_found:,} ({systematic_found/entries_processed*100:.1f}%)")
        print(f"‚úÖ Successfully enriched: {enriched_count:,}")
        print(f"‚ö° Enrichment rate: {enriched_count/elapsed_time*60:.0f} enriched/minute")

        with self.stats_lock:
            print(f"üíæ Instant lookups performed: {self.stats['instant_lookups']:,}")
            print(f"üîç Lookup rate: {self.stats['instant_lookups']/elapsed_time*60:.0f} lookups/minute")
            print(f"üåç Bodies found: {self.stats['body_found']:,} ({self.stats['body_found']/max(1,enriched_count)*100:.1f}%)")
            print(f"‚ùå Bodies not found: {self.stats['body_not_found']:,}")

        print(f"üìÅ Output file: {output_file}")
        print(f"üìä Output size: {output_file.stat().st_size / (1024*1024):.1f} MB")
        print("‚ö° Ultra-fast optimizations: Pre-loaded index, instant lookups, batch processing")

        return enriched_count

def main():
    """Main execution function."""
    import argparse

    parser = argparse.ArgumentParser(description='Ultra-fast comprehensive species analysis enrichment')
    parser.add_argument('--max-entries', type=int, help='Limit number of entries to process')
    parser.add_argument('--workers', type=int, default=12, help='Number of worker threads (default: 12)')
    args = parser.parse_args()

    codex_path = Path('Databases/codex.json/codex.jsonl')
    index_path = Path('Databases/complete_sector_index.pkl')
    output_dir = Path('output/enriched_codex_data')

    if not codex_path.exists():
        print(f"‚ùå Codex JSONL file not found: {codex_path}")
        return

    if not index_path.exists():
        print(f"‚ùå Sector index not found: {index_path}")
        print(f"üîß Please run: python scripts/build_full_sector_index.py")
        return

    # Create ultra-fast enricher
    enricher = UltraFastEnricher(index_path, output_dir, num_workers=args.workers)

    # Create output file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    output_file = output_dir / f"codex_species_analysis_ultrafast_{timestamp}.jsonl"

    # Run ultra-fast comprehensive enrichment
    enriched_count = enricher.stream_process_codex_ultrafast(
        codex_path=codex_path,
        output_file=output_file,
        max_entries=args.max_entries
    )

    print(f"\nüéâ Ultra-fast species analysis enrichment complete! {enriched_count:,} entries enriched.")

if __name__ == "__main__":
    main()